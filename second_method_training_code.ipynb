{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Ym50e7ijHI"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I3XPSk6ijHJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "!pip install unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Model Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224,
          "referenced_widgets": [
            "8e1804ea6f5b486e9f933cbfa4ee75a4",
            "b94924121ab142cd94711888282dc5e7",
            "f183d9d93b0b4d4196522043152a82db",
            "04aeb49804ef4fe9b8064fbcdb905709",
            "91bb034cb9a449fca2ea69a9e2fdda90",
            "27ba5e5b74034507a3585e708f5764cd",
            "c0bcab7a3d094532a267f68d8441eb2d",
            "1bdc4953166e4b34ba1f394b01442ab5",
            "01d0eabca0154c1aa0dbc025a3dcedf7",
            "9af47aa12e7c484390aec580ce79e563",
            "21351ab92db34c2da5769b7623088bdf",
            "174206da79cb4f6a95d9f4357f8cb26b",
            "d66d2a7222824faab9f4ae0d88cadad2",
            "f56191075ac8481f84fae1e984edc111",
            "25d16b661431427a8cccba3d11a68d4c",
            "2512cb5c934c48b6b1734ab323a81f0e",
            "508671cf4b0d4c3d966e1f8ba8e977f4",
            "43d0c8b66ffa4ac2a8267ffe8b56c96f",
            "cd010321d0fe42d180f0650ba07b41f2",
            "86eb3da0e574405daf1ea6f7cbb841e8",
            "781d2fe76a674391b0062964936544be",
            "bc071f665b4d4231bd2c124ff92645d5"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "067acb4f-9450-4d38-d4f3-193f400d76ab"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel\n",
        "import torch\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2.5-VL-7B-Instruct-bnb-4bit\",\n",
        "    load_in_4bit = True,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True,\n",
        "    finetune_language_layers   = True,\n",
        "    finetune_attention_modules = True,\n",
        "    finetune_mlp_modules       = True,\n",
        "    r = 16,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "### Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "f2f9365475d140e0bd4dc47ec7c799ab",
            "aed78b348b3c4662979301bdd2208bca",
            "09c42c375e96401c850b60318492b117",
            "0ab00ef8c43c4650897a8546fd470463",
            "d243746fe5c44dd3964edbcfd00fb690",
            "00ab1c14474041ba9b740669f7faac1f",
            "7fc4ccfee4a84808a7bea6056dccfaef",
            "458f2722640b47fd874171494658f22f",
            "1024fcf007824b159da3c212306f58e5",
            "c53af07aeecb4551937d0c846b1bbf36",
            "021e670f1f7a4a1d9cc2508dfe0ac1a2"
          ]
        },
        "id": "ofHO-rY31xgr",
        "outputId": "778046ea-092e-43b0-875f-16ed3b73292f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2f9365475d140e0bd4dc47ec7c799ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['img_id', 'image', 'mask', 'width', 'height', 'label'],\n",
            "        num_rows: 844\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "!!for i in {0..4}; do wget -q https://huggingface.co/datasets/saberzl/SID_Set/resolve/main/data/train-$(printf \"%05d\" $i)-of-00249.parquet; done\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "file_names = [f\"train-{i:05d}-of-00249.parquet\" for i in range(5)]\n",
        "\n",
        "# ['train-00000-of-00249.parquet', 'train-00001-of-00249.parquet', 'train-00002-of-00249.parquet', 'train-00003-of-00249.parquet', 'train-00004-of-00249.parquet']\n",
        "\n",
        "dataset = load_dataset(\"parquet\", data_files=file_names)\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDcMcwzJzssT"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Ra2E1Wlqac4"
      },
      "outputs": [],
      "source": [
        "# remove images with label 2\n",
        "dataset_no_label_2 = train_dataset.filter(lambda example: example['label'] != 2, num_proc=12)\n",
        "\n",
        "# choose only images with width 1024\n",
        "dataset_1024_only = dataset_no_label_2.filter(lambda example: example['width'] == 1024, num_proc=12)\n",
        "\n",
        "\n",
        "dataset_0_1024 = dataset_1024_only.filter(lambda e: e['label'] == 0,  num_proc=12)\n",
        "dataset_1_1024 = dataset_1024_only.filter(lambda e: e['label'] == 1,  num_proc=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEabehDJsIeb"
      },
      "outputs": [],
      "source": [
        "# undersample label 1 to match label 0 count\n",
        "dataset_label_1_undersampled = dataset_1_1024.select(\n",
        "    range(len(dataset_1_1024)),\n",
        ").shuffle(seed=42).select(range(1134))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II5LsDrisprk"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets\n",
        "\n",
        "dataset_dengelenmis_final = concatenate_datasets([\n",
        "    dataset_0_1024,                   # 1134 item\n",
        "    dataset_label_1_undersampled       # 1134 item\n",
        "])\n",
        "\n",
        "# shuffle dataset for better training\n",
        "dataset_dengelenmis_final = dataset_dengelenmis_final.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_2K0Wg8s_El"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import random\n",
        "import pandas as pd\n",
        "from datasets import ClassLabel, load_dataset, Features, Value\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Cv/SID_Set\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "num_cores = os.cpu_count() or 2\n",
        "\n",
        "print(f\"Starting, the path is: {SAVE_DIR}\")\n",
        "\n",
        "# convert label colon to ClassLabel to work with\n",
        "yeni_ozellikler = dataset_dengelenmis_final.features.copy()\n",
        "yeni_ozellikler[\"label\"] = ClassLabel(num_classes=2)\n",
        "dataset_dengelenmis_final = dataset_dengelenmis_final.cast(yeni_ozellikler)\n",
        "\n",
        "#%70 Train, %15 Validation, %15 Test\n",
        "bolunmus = dataset_dengelenmis_final.train_test_split(\n",
        "    test_size=0.3, seed=42, stratify_by_column=\"label\"\n",
        ")\n",
        "train_split = bolunmus[\"train\"]\n",
        "temp_split = bolunmus[\"test\"]\n",
        "\n",
        "ikinci_bolme = temp_split.train_test_split(\n",
        "    test_size=0.5, seed=42, stratify_by_column=\"label\"\n",
        ")\n",
        "val_split = ikinci_bolme[\"train\"]\n",
        "test_split = ikinci_bolme[\"test\"]\n",
        "\n",
        "print(f\"Splits ready, Train: {len(train_split)}, Val: {len(val_split)}, Test: {len(test_split)}\")\n",
        "\n",
        "\n",
        "TARGET_SIZE = (512, 512)\n",
        "\n",
        "def compress_image(image):\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "    buffer = io.BytesIO()\n",
        "\n",
        "    q = random.randint(50, 95)\n",
        "    image.save(buffer, format=\"JPEG\", quality=q, optimize=True)\n",
        "    buffer.seek(0)\n",
        "    return Image.open(buffer)\n",
        "\n",
        "train_transforms_base = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "])\n",
        "\n",
        "# apply augmentation only to train data\n",
        "def train_augment(image):\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    img = train_transforms_base(image)\n",
        "\n",
        "    resizer = transforms.RandomResizedCrop(\n",
        "        size=TARGET_SIZE,\n",
        "        scale=(0.4, 1.0),\n",
        "        ratio=(0.8, 1.25),\n",
        "        interpolation=transforms.InterpolationMode.BILINEAR\n",
        "    )\n",
        "    img = resizer(img)\n",
        "\n",
        "    img = compress_image(img)\n",
        "    return img\n",
        "\n",
        "def val_test_process(image):\n",
        "    if image.mode != 'RGB':\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "    return image\n",
        "\n",
        "# images are saved to disk to not wait for augmentation next time\n",
        "def process_and_save(batch, split_name=\"train\"):\n",
        "    filenames_list = []\n",
        "    labels_list = []\n",
        "    splits_list = []\n",
        "\n",
        "    for i in range(len(batch[\"image\"])):\n",
        "        img = batch[\"image\"][i]\n",
        "        lbl = batch[\"label\"][i]\n",
        "\n",
        "        base_id = batch[\"img_id\"][i] if \"img_id\" in batch else f\"{split_name}_{i}\"\n",
        "        safe_base_id = str(base_id).replace(\"/\", \"_\")\n",
        "\n",
        "        if split_name == \"train\":\n",
        "            final_img = train_augment(img)\n",
        "            filename = f\"{safe_base_id}_aug.jpg\"\n",
        "        else:\n",
        "            final_img = val_test_process(img)\n",
        "            filename = f\"{safe_base_id}_std.jpg\" # std = standard\n",
        "\n",
        "        save_path = os.path.join(SAVE_DIR, filename)\n",
        "\n",
        "        try:\n",
        "            final_img.save(save_path, \"JPEG\")\n",
        "            filenames_list.append(filename)\n",
        "            labels_list.append(lbl)\n",
        "            splits_list.append(split_name)\n",
        "        except Exception as e:\n",
        "            print(f\"Hata: {e}\")\n",
        "\n",
        "    return {\n",
        "        \"file_name\": filenames_list,\n",
        "        \"label\": labels_list,\n",
        "        \"split\": splits_list\n",
        "    }\n",
        "\n",
        "all_metadata_dfs = []\n",
        "\n",
        "datasets_to_process = [\n",
        "    (\"train\", train_split),\n",
        "    (\"validation\", val_split),\n",
        "    (\"test\", test_split)\n",
        "]\n",
        "\n",
        "for name, dset in datasets_to_process:\n",
        "    print(f\"starting to save images to disk\")\n",
        "\n",
        "    processed = dset.map(\n",
        "        process_and_save,\n",
        "        batched=True,\n",
        "        batch_size=50,\n",
        "        num_proc=num_cores,\n",
        "        fn_kwargs={\"split_name\": name},\n",
        "        remove_columns=dset.column_names,\n",
        "        desc=f\"Saving {name}\"\n",
        "    )\n",
        "\n",
        "    df_part = processed.to_pandas()\n",
        "    all_metadata_dfs.append(df_part)\n",
        "\n",
        "print(\"\\nMetadata step..\")\n",
        "full_metadata = pd.concat(all_metadata_dfs, ignore_index=True)\n",
        "metadata_path = os.path.join(SAVE_DIR, \"metadata.csv\")\n",
        "full_metadata.to_csv(metadata_path, index=False)\n",
        "print(f\"saved to {metadata_path}\")\n",
        "print(f\"all data {len(full_metadata)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByROLM8_pDq3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import ClassLabel, load_dataset, Features, Value\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/Cv/SID_Set\"\n",
        "print(\"\\n reading images from disk\")\n",
        "\n",
        "dataset_final = load_dataset(\"imagefolder\", data_dir=SAVE_DIR)\n",
        "dataset_final = dataset_final.cast_column(\"label\", ClassLabel(num_classes=2))\n",
        "full_data = dataset_final[\"train\"]\n",
        "\n",
        "print(\"starting splitting images to sets(test, train..)\")\n",
        "\n",
        "split_column = np.array(full_data[\"split\"])\n",
        "\n",
        "train_indices = np.where(split_column == \"train\")[0]\n",
        "val_indices = np.where(split_column == \"validation\")[0]\n",
        "test_indices = np.where(split_column == \"test\")[0]\n",
        "\n",
        "train_dataset = full_data.select(train_indices)\n",
        "validation_dataset = full_data.select(val_indices)\n",
        "test_dataset = full_data.select(test_indices)\n",
        "\n",
        "cols_to_keep = [\"image\", \"label\"]\n",
        "remove_cols = [c for c in full_data.column_names if c not in cols_to_keep]\n",
        "\n",
        "train_dataset = train_dataset.remove_columns(remove_cols)\n",
        "validation_dataset = validation_dataset.remove_columns(remove_cols)\n",
        "test_dataset = test_dataset.remove_columns(remove_cols)\n",
        "\n",
        "print(f\"Train Dataset: {len(train_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbVBgmVERUuM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def show_random_batch(dataset, num_images=30, cols=6):\n",
        "    indices = random.sample(range(len(dataset)), num_images)\n",
        "\n",
        "    rows = (num_images + cols - 1) // cols\n",
        "\n",
        "    plt.figure(figsize=(20, 3 * rows))\n",
        "\n",
        "    for i, idx in enumerate(indices):\n",
        "        item = dataset[idx]\n",
        "        image = item['image']\n",
        "        label = item['label']\n",
        "\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"Label: {label}\\nIdx: {idx}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_random_batch(train_dataset, num_images=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPXzJZzHEgXe"
      },
      "outputs": [],
      "source": [
        "# convert dataset to proper format for LLM\n",
        "instruction = \"\"\"Real/Fake?\"\"\"\n",
        "def convert_to_conversation(sample):\n",
        "    label_text = \"Real\" if sample[\"label\"] == 0 else \"Fake\"\n",
        "    conversation = [\n",
        "        { \"role\": \"user\",\n",
        "          \"content\" : [\n",
        "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
        "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
        "        },\n",
        "        { \"role\" : \"assistant\",\n",
        "          \"content\" : [\n",
        "            {\"type\" : \"text\",  \"text\"  : label_text } ]\n",
        "        },\n",
        "    ]\n",
        "    return { \"messages\" : conversation }\n",
        "converted_dataset = [convert_to_conversation(sample) for sample in train_dataset]\n",
        "converted_dataset_valid = [convert_to_conversation(sample) for sample in validation_dataset]\n",
        "converted_dataset_test = [convert_to_conversation(sample) for sample in test_dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahVyzSPquawh"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyLqn0aQskyi"
      },
      "outputs": [],
      "source": [
        "from transformers import EvalPrediction\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pdb as pd\n",
        "\n",
        "# required token ids, end of message, \"Fake\" text token, \"Real\" text token\n",
        "END_OF_MESSAGE_ID = 151645 # <|im_end|>\n",
        "FAKE_ID = 52317\n",
        "REAL_ID = 12768\n",
        "CRITICAL_IDS = {FAKE_ID, REAL_ID}\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    # this runs after each batch\n",
        "\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "\n",
        "    return logits.argmax(dim=-1)  # (batch, seq_len, vocab) -> (batch, seq_len)\n",
        "\n",
        "def compute_classification_metrics(p: EvalPrediction):\n",
        "    \"\"\"\n",
        "    Predictions zaten argmax'lanmış geliyor (preprocess_logits_for_metrics sayesinde)\n",
        "    \"\"\"\n",
        "\n",
        "    # use tensor\n",
        "    preds = torch.from_numpy(p.predictions)\n",
        "    labels_tensor = torch.from_numpy(p.label_ids)\n",
        "\n",
        "    # apply mask to remove unnecessary token\n",
        "    mask = labels_tensor != -100\n",
        "\n",
        "    # find the token before last token with id \"151645\"(im_end)\n",
        "    def find_decision_token(token_seq, seq_mask):\n",
        "        # find decision token using mask\n",
        "\n",
        "        # choose unmasked tokens\n",
        "        masked_seq = token_seq[seq_mask]\n",
        "\n",
        "        indices_of_end_token = (masked_seq == END_OF_MESSAGE_ID).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        if indices_of_end_token.size(0) > 0:\n",
        "            last_end_token_index = indices_of_end_token[-1]\n",
        "            karar_index = last_end_token_index - 1\n",
        "\n",
        "            if karar_index >= 0:\n",
        "                return masked_seq[karar_index]\n",
        "        return None\n",
        "\n",
        "    final_preds_list = []\n",
        "    final_labels_list = []\n",
        "\n",
        "    # run on each item\n",
        "    for i in range(preds.size(0)):\n",
        "        seq_mask = mask[i]\n",
        "\n",
        "        label_token = find_decision_token(labels_tensor[i], seq_mask)\n",
        "        pred_token = find_decision_token(preds[i], seq_mask)\n",
        "\n",
        "        # compare tokens\n",
        "        if label_token is not None and label_token.item() in CRITICAL_IDS:\n",
        "            if pred_token is not None:\n",
        "                final_labels_list.append(label_token.item())\n",
        "                final_preds_list.append(pred_token.item())\n",
        "            else:\n",
        "                final_labels_list.append(label_token.item())\n",
        "                final_preds_list.append(-999)\n",
        "\n",
        "    if not final_labels_list:\n",
        "        return {\"accuracy\": 0.0, \"eval_RealFake_count\": 0}\n",
        "\n",
        "    accuracy = accuracy_score(final_labels_list, final_preds_list)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy),\n",
        "        \"eval_RealFake_count\": int(len(final_labels_list))\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95_Nn-89DhsL",
        "outputId": "505eb3a5-8698-4513-8d74-c9406517c58e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Model does not have a default image size - using 512\n"
          ]
        }
      ],
      "source": [
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "FastVisionModel.for_training(model)\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.0,\n",
        ")\n",
        "\n",
        "# !!!! This model is first trained without focal loss for one epoch like the model of Method 1 but then trained with focal loss for 2 epoch\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
        "    train_dataset = converted_dataset,\n",
        "    eval_dataset = converted_dataset_valid,\n",
        "    callbacks = [early_stopping],\n",
        "    compute_metrics = compute_classification_metrics,\n",
        "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
        "    loss_type='focal', # there was no this line in the previous training step\n",
        "    loss_params={'alpha': 0.75, 'gamma': 2.0}, # this line also didn't exist in the previous training step\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = 8,\n",
        "        per_device_eval_batch_size = 10,\n",
        "        gradient_accumulation_steps = 5,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 2, # it was 1 in the previous training step\n",
        "        # learning_rate = 1e-4, # previous learning reate\n",
        "        learning_rate = 5e-5,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        # lr_scheduler_type = \"linear\", # previous scheduler\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "        eval_steps = 10,\n",
        "        metric_for_best_model = \"accuracy\",\n",
        "        eval_strategy=\"steps\",\n",
        "        load_best_model_at_end = True,\n",
        "        remove_unused_columns = False,\n",
        "        dataset_text_field = \"\",\n",
        "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
        "        max_length = 2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "071cc7ae-66ac-4e84-99a9-6aaaaefe714c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,587 | Num Epochs = 2 | Total steps = 80\n",
            "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 5\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 5 x 1) = 40\n",
            " \"-____-\"     Trainable parameters = 51,521,536 of 8,343,688,192 (0.62% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80/80 22:07, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Realfake Count</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.017300</td>\n",
              "      <td>0.007247</td>\n",
              "      <td>340</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.006000</td>\n",
              "      <td>0.002997</td>\n",
              "      <td>340</td>\n",
              "      <td>0.985294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.004800</td>\n",
              "      <td>0.002826</td>\n",
              "      <td>340</td>\n",
              "      <td>0.982353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>0.001567</td>\n",
              "      <td>340</td>\n",
              "      <td>0.988235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>0.000985</td>\n",
              "      <td>340</td>\n",
              "      <td>0.991176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>340</td>\n",
              "      <td>0.997059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>340</td>\n",
              "      <td>0.994118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>0.000904</td>\n",
              "      <td>340</td>\n",
              "      <td>0.994118</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "e3775863-afa8-4ba2-a9d4-e7e1c9005551"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"fake_image_detector_99_3\")\n",
        "tokenizer.save_pretrained(\"fake_image_detector_99_3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCqI0J-E4jw0"
      },
      "source": [
        "### Inference Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACGPFn0O9e9W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "actual_tokenizer = tokenizer.tokenizer\n",
        "\n",
        "# Get token IDs for \"Fake\" and \"Real\"\n",
        "fake_token_id = actual_tokenizer.encode(\"Fake\", add_special_tokens=False)[0]\n",
        "real_token_id = actual_tokenizer.encode(\"Real\", add_special_tokens=False)[0]\n",
        "\n",
        "print(f\"Fake token ID: {fake_token_id}, Real token ID: {real_token_id}\")  # Debug\n",
        "\n",
        "for num in range(len(converted_dataset_test)):\n",
        "    image = converted_dataset_test[num]['messages'][0]['content'][1]['image']\n",
        "    act_label = converted_dataset_test[num]['messages'][1]['content'][0]['text']\n",
        "\n",
        "    instruction = \"\"\"Real/Fake?\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": instruction}\n",
        "        ]}\n",
        "    ]\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate to get prediction with scores\n",
        "    outputs = model.generate(**inputs, max_new_tokens=128,\n",
        "                            do_sample=False,\n",
        "                            output_scores=True, return_dict_in_generate=True)\n",
        "\n",
        "    generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "    generated_text = actual_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "    prediction = generated_text.strip().split()[0].lower() if generated_text.strip() else \"\"\n",
        "    print(generated_text)\n",
        "    # Get probabilities for the first token (Fake or Real)\n",
        "    first_token_logits = outputs.scores[0][0]  # [vocab_size]\n",
        "    probabilities = F.softmax(first_token_logits, dim=-1)\n",
        "\n",
        "    fake_prob = probabilities[fake_token_id].item() * 100\n",
        "    real_prob = probabilities[real_token_id].item() * 100\n",
        "\n",
        "    actual = act_label.lower().strip()\n",
        "\n",
        "    #this part has threshold to prevent fake bias\n",
        "    pred_prob = fake_prob if prediction == \"fake\" else real_prob\n",
        "    if prediction == \"fake\" and pred_prob < 80:\n",
        "        prediction = \"real\"\n",
        "        pred_prob = real_prob\n",
        "\n",
        "    is_correct = prediction == actual\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    total += 1\n",
        "\n",
        "\n",
        "    status = \"✅ CORRECT\" if is_correct else \"❌ INCORRECT\"\n",
        "    print(f\" {num+1}/{len(converted_dataset_valid)}: {status}\")\n",
        "    print(f\"  {act_label} | {prediction} ({pred_prob:.2f}%)\")\n",
        "    print(f\"CORRECT/TOTAL PREDICTION {correct}/{total} ({100*correct/total:.2f}%)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(f\"FINAL ACCURACY: {correct}/{total} = {100*correct/total:.2f}%\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf3kzGKjfuPi"
      },
      "source": [
        "### Other Datasets Testing (ImageNet and Dalle3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO5YJvJCFm-7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Cv/dalle3_random_100\"\n",
        "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "actual_tokenizer = tokenizer.tokenizer\n",
        "fake_token_id = actual_tokenizer.encode(\"Fake\", add_special_tokens=False)[0]\n",
        "real_token_id = actual_tokenizer.encode(\"Real\", add_special_tokens=False)[0]\n",
        "\n",
        "def get_label_from_filename(filename):\n",
        "    return \"fake\"\n",
        "\n",
        "image_files = []\n",
        "if os.path.exists(dataset_path):\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        if ext in valid_extensions:\n",
        "            image_files.append(os.path.join(dataset_path, filename))\n",
        "else:\n",
        "    print(f\"No folder found!\")\n",
        "\n",
        "\n",
        "if len(image_files) > 0:\n",
        "    print(f\"Total images: {len(image_files)}. Starting..\\n\")\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for img_path in image_files:\n",
        "        filename = os.path.basename(img_path)\n",
        "        true_label = get_label_from_filename(filename)\n",
        "\n",
        "        if true_label is None:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            instruction = \"Real/Fake?\"\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": instruction}\n",
        "                ]}\n",
        "            ]\n",
        "            input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "            inputs = tokenizer(\n",
        "                image,\n",
        "                input_text,\n",
        "                add_special_tokens=False,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=128,\n",
        "                                        use_cache=True, do_sample=False,\n",
        "                                        output_scores=True, return_dict_in_generate=True)\n",
        "\n",
        "            generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "            generated_text = actual_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "            raw_pred = generated_text.strip().split()[0].lower() if generated_text.strip() else \"unknown\"\n",
        "\n",
        "            first_token_logits = outputs.scores[0][0]\n",
        "            probabilities = F.softmax(first_token_logits, dim=-1)\n",
        "\n",
        "            fake_prob = probabilities[fake_token_id].item() * 100\n",
        "            real_prob = probabilities[real_token_id].item() * 100\n",
        "\n",
        "            if \"fake\" in raw_pred:\n",
        "                prediction = \"fake\"\n",
        "                current_prob = fake_prob\n",
        "            elif \"real\" in raw_pred:\n",
        "                prediction = \"real\"\n",
        "                current_prob = real_prob\n",
        "            else:\n",
        "                prediction = \"unknown\"\n",
        "                current_prob = 0.0\n",
        "\n",
        "            # apply threshold to prevent fake bias\n",
        "            threshold_msg = \"\"\n",
        "            if prediction == \"fake\" and fake_prob < 80.0:\n",
        "                prediction = \"real\"\n",
        "                current_prob = real_prob\n",
        "                threshold_msg = \" [fixed with threshold]\"\n",
        "\n",
        "            y_true.append(true_label)\n",
        "            y_pred.append(prediction)\n",
        "\n",
        "            match_icon = \"✅\" if true_label == prediction else \"❌\"\n",
        "\n",
        "            print(f\"{match_icon} {filename} | True: {true_label} -> Pred: {prediction.upper()} ({current_prob:.2f}%){threshold_msg}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception!!! {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    if len(y_true) > 0:\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        print(f\"\\nAccuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "        labels = [\"fake\", \"real\"]\n",
        "\n",
        "        try:\n",
        "            cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "            cm_df = pd.DataFrame(cm, index=[f\"Act {l}\" for l in labels],\n",
        "                                 columns=[f\"Pred {l}\" for l in labels])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(cm_df)\n",
        "\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=labels))\n",
        "        except Exception as e:\n",
        "            print(f\"Exception \", e)\n",
        "\n",
        "    else:\n",
        "        print(\"No data!\")\n",
        "\n",
        "else:\n",
        "    print(\"No data in folder! Check path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2Ar7b7BMvCB"
      },
      "outputs": [],
      "source": [
        "# this code is same as the above, the difference is this code have compression of images like on social media\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import io\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Cv/dalle3_random_100\"\n",
        "valid_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
        "\n",
        "FastVisionModel.for_inference(model)\n",
        "\n",
        "actual_tokenizer = tokenizer.tokenizer\n",
        "fake_token_id = actual_tokenizer.encode(\"Fake\", add_special_tokens=False)[0]\n",
        "real_token_id = actual_tokenizer.encode(\"Real\", add_special_tokens=False)[0]\n",
        "\n",
        "def get_label_from_filename(filename):\n",
        "    return \"fake\"\n",
        "\n",
        "image_files = []\n",
        "if os.path.exists(dataset_path):\n",
        "    for filename in os.listdir(dataset_path):\n",
        "        ext = os.path.splitext(filename)[1].lower()\n",
        "        if ext in valid_extensions:\n",
        "            image_files.append(os.path.join(dataset_path, filename))\n",
        "else:\n",
        "    print(f\"No folder found!\")\n",
        "\n",
        "# --- ANA MANTIK ---\n",
        "if len(image_files) > 0:\n",
        "    print(f\"Total images: {len(image_files)}. Starting..\\n\")\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for img_path in image_files:\n",
        "        filename = os.path.basename(img_path)\n",
        "        true_label = get_label_from_filename(filename)\n",
        "\n",
        "        if true_label is None:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "            # this step to simulate compression on social media\n",
        "            buffer = io.BytesIO()\n",
        "            image.save(buffer, format=\"JPEG\", quality=65, optimize=True)\n",
        "            buffer.seek(0)\n",
        "            image = Image.open(buffer)\n",
        "\n",
        "            instruction = \"Real/Fake?\"\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"image\"},\n",
        "                    {\"type\": \"text\", \"text\": instruction}\n",
        "                ]}\n",
        "            ]\n",
        "            input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "            inputs = tokenizer(\n",
        "                image,\n",
        "                input_text,\n",
        "                add_special_tokens=False,\n",
        "                return_tensors=\"pt\",\n",
        "            ).to(\"cuda\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(**inputs, max_new_tokens=128,\n",
        "                                        use_cache=True, do_sample=False,\n",
        "                                        output_scores=True, return_dict_in_generate=True)\n",
        "\n",
        "            generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "            generated_text = actual_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "            raw_pred = generated_text.strip().split()[0].lower() if generated_text.strip() else \"unknown\"\n",
        "\n",
        "            first_token_logits = outputs.scores[0][0]\n",
        "            probabilities = F.softmax(first_token_logits, dim=-1)\n",
        "\n",
        "            fake_prob = probabilities[fake_token_id].item() * 100\n",
        "            real_prob = probabilities[real_token_id].item() * 100\n",
        "\n",
        "            if \"fake\" in raw_pred:\n",
        "                prediction = \"fake\"\n",
        "                current_prob = fake_prob\n",
        "            elif \"real\" in raw_pred:\n",
        "                prediction = \"real\"\n",
        "                current_prob = real_prob\n",
        "            else:\n",
        "                prediction = \"unknown\"\n",
        "                current_prob = 0.0\n",
        "\n",
        "            # threshold part to prevent fake bias\n",
        "            threshold_msg = \"\"\n",
        "            if prediction == \"fake\" and fake_prob < 80.0:\n",
        "                prediction = \"real\"\n",
        "                current_prob = real_prob\n",
        "                threshold_msg = \" [fixed with threshold]\"\n",
        "\n",
        "            y_true.append(true_label)\n",
        "            y_pred.append(prediction)\n",
        "\n",
        "            match_icon = \"✅\" if true_label == prediction else \"❌\"\n",
        "\n",
        "            print(f\"{match_icon} {filename} | (JPEG-65) | True: {true_label} -> Pred: {prediction.upper()} ({current_prob:.2f}%){threshold_msg}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception!!! {e}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"RESULTS (With Compression)\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    if len(y_true) > 0:\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        print(f\"\\nAccuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
        "\n",
        "        labels = [\"fake\", \"real\"]\n",
        "        try:\n",
        "            cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "            cm_df = pd.DataFrame(cm, index=[f\"Act {l}\" for l in labels],\n",
        "                                 columns=[f\"Pred {l}\" for l in labels])\n",
        "            print(\"\\nConfusion Matrix:\")\n",
        "            print(cm_df)\n",
        "\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred, target_names=labels))\n",
        "        except Exception as e:\n",
        "            print(f\"Exception \", e)\n",
        "\n",
        "    else:\n",
        "        print(\"No data!\")\n",
        "\n",
        "else:\n",
        "    print(\"No data in folder! Check path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9wD5Ip11eBF"
      },
      "source": [
        "### Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oxZkAGc1m1o"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"fake_image_detector_99_3\")\n",
        "tokenizer.save_pretrained(\"fake_image_detector_99_3\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "y4Ym50e7ijHI",
        "9xLDGk41C7IF",
        "vITh0KVJ10qX",
        "ahVyzSPquawh",
        "XCqI0J-E4jw0",
        "L9wD5Ip11eBF"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3 (System)",
      "language": "python",
      "name": "system-python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
