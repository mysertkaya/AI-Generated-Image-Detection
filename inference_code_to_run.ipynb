{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddi-QEQ5AY80"
      },
      "source": [
        "# Inference Code\n",
        "\n",
        "This notebook implements the inference pipeline.\n",
        "\n",
        "## Overview\n",
        "- **Model**: `unsloth/Qwen2.5-VL-7B-Instruct`\n",
        "- **Configuration**: 4-bit quantization, LoRA fine-tuning.\n",
        "- **Task**: Binary Classification (Real vs. Fake) on high-quality images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cbdmtrtZAY82",
        "outputId": "83b99351-48f7-4df3-e027-61d1820806fb"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install unsloth torch torchvision pillow gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzj4vIF7AY83",
        "outputId": "1b014ec0-5cef-41b0-a3e2-698828512136"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastVisionModel\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HpnIwf5Apvz",
        "outputId": "d3aa2757-eecf-4fd6-fc6e-83d927522f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2178db6ecc064230a79c96598ff10053",
            "f9e5093407aa429ea0a17dfc01696e36",
            "2f8a5ee22f6a41168d226b39fc425bad",
            "7947ed640bcc428b9f1f27775705662f",
            "7e275828acb043059eba6119f83deeb2",
            "783f8207d2314cc8b8636447289ea3d9",
            "b49af2830dac427f8ddbe65ad4c36b77",
            "fa0598240f0f4d6c9fe337d9b419c5cc",
            "60083927bd1841db88ee7071d4c83a87",
            "3ab7fc130408455c9ca38fe908b30251",
            "911ef57e33fd4eff927b7a5933d4832b",
            "2bd66e331a03454c8e38f441c90c8759",
            "7ed931f39e2d41b2b3042cf3c15eb394",
            "02ba8bb157124c02a15e394418c0c1e5",
            "ecea3b3dfe444e9ebbbc3336760b8784",
            "67e1faa2296c455fafa54dfe6b0acea9",
            "f0a84b389daf4848844489d7e8c0646a",
            "847434f5714f4da4968ad7cee6637a94",
            "614d6c73edb947eba90c4d3ec37d11a4",
            "16df40f35792444a85d9d59817f22105",
            "5f337b80aba24d879b4efbddd1640922",
            "7c3a420d82194244b77723cf89c1d5a5",
            "7474f1cb8b9d4042b78f94cf6e2f6cd7",
            "25901adf86dc4594aacd0dcf8ca559a0",
            "316db7b56f8e40ecad44ac044577a836",
            "69f0ad5de4634cb4a1065506a97647d3",
            "a6a39cd9b28942c186c6f9e83ea5d743",
            "564553079cbc4e20becc7dd7e3ea917f",
            "7c1c44122d7d483dba192fb8ced36c79",
            "6495d0581967496a95f5f785cef42cf5",
            "3c7947b6108f43f89226ccc47ce71f42",
            "38470b542d6b4dce9123184a4aaee1f5",
            "8aa342ea0e6449168c196400595de3ec",
            "60f386cb2d6f40f690d791d6d3ef0cb1",
            "406ae093c304488f99af918fda072cc2",
            "f92b620a5dd74c31a7b3ccf58c9c7957",
            "91b5a530f09c4fababb9e96ba5f292e9",
            "7a2e5f2729e645e580d6ce170fc7a7c7",
            "6e6c99aaaf4b4997b5e470c35769f2e0",
            "94aaa903074945a894316458d191d10f",
            "fcdbf6169f004083aae9c31451ef7560",
            "a445434a2c4c46f7a17ce2502efcc10b",
            "f313ddf02d8145d58efcc97a993a582c",
            "0402649903024403a6c7a0285a3b6170",
            "0e36d89ef03a47cda53a4c3f18274e37",
            "3c8638583a2246f5ad0a56a990145975",
            "672b7246dfce48e79e95d21f2747abe4",
            "3081b303d225471b8dd8a25fd511f628",
            "9420d150c32a4559b5cfcf73d12e2650",
            "050a1846ba5e4df1b9e0f2367777c2c0",
            "b149e0a7bb0443d7a041f76b614cc149",
            "ceb549a0e24d4abdaa8aa398a9bfac37",
            "7557bafe94bf48df915b25e7dd899b1f",
            "c033f3055e8a47559e5709e6ac273864",
            "c1fa19d26c654f57b2f0e30fcb5c6da6",
            "b1455a7398714532bb13d897d8b74fbb",
            "6396c242af7a4bc785fe5280532621a5",
            "32194148e45d48e6a9b88086b8904dac",
            "f639b56303b141c180066f8700e98602",
            "81a4cfe22b1b4ffdbc0e7b4c51d0a042",
            "848ff650086b46c18ffc802ae514b806",
            "7f46b43fa4ff45ff8eb3e6f5d55c398c",
            "3833c43b40df4d98a64117ecf26f4e12",
            "a0bfec1c851e4df0a95834fc0272712e",
            "80df07d5aa544f66beacd831fbe32d4a",
            "77e7728ed76f4cf6992cb7910f8124b9",
            "8ffaf5a68ae54cabb62612532a929c31",
            "10c13bfc8cc8436598581bfc310fb9fc",
            "1295eb8d799e46038d9b57fc6a285df0",
            "bb8fb5610f03427a9a3fa782c743d2d8",
            "79cfcd789a2a44ed947f181619c46b10",
            "b4012a0134d143d59548c77426b47fd9",
            "856230866f5a4dbf95b5a10fd568fea0",
            "66aef02cc5db47e9a21eedd137f5d950",
            "d8f6e1acdfd149ba90a1d73f5582bd7c",
            "d5800597d7354f25abe88ca0f414064e",
            "ffc857e883fc4bc3a7930a95b016a6b6",
            "5cd84c729a1649a98dabe8a8c21602be",
            "c19811bf0c434262add7a12af13f88e2",
            "ead2d9771be442d7a1a35d765f2342c6",
            "36f36e3d44074060a1568958c600723f",
            "5f9fbb2bea6446c3adcc1243a368cf06",
            "6895c85e52f7468baada9367ed5487c1",
            "1a50c61934a744b1bdfaa023e0e58b9e",
            "50d3369eaf7348b3bde4535b5d2641ad",
            "3feab647016d437e8e99d4d00e0c95e8",
            "ee7959315e2641bc9f8904aa1fae9309",
            "5b735060f73e41be9c925c8f853388ca",
            "624329dfacb34a5abe9681884607ab4f",
            "97c7c2ca72bd4b84be8dd0e9e9031887",
            "de2ce790f0b34c83a0c17515b2b891e0",
            "ea43ae6ac80e4a0489b486f883d1d735",
            "65e61c02f64e4ad5b5be80de55712a3f",
            "ddc800b669654fed92e291fed7d97ae3",
            "4361f947e19f4af5b48d57d809127150",
            "5d68de79d1df496cb834d791368f0f9b",
            "f9ed867188334bb28aa05866333d4c70",
            "c17ec3ab0f304825b59788a926b28ff1",
            "1f4c7a4540e54b33bd867d20b2a0e61c",
            "fa8121ed867044058aaf0b41a29c112c",
            "ae2bb4285cf544928e5516d2e0172dd8",
            "f7496265ec6d4b0bb37ad27140591c16",
            "959c293f381a4c2789267f3170a99325",
            "855ca019033e4b6e8b030dbcbf1429e1",
            "11d2b6f0d4f8471a96f018eb876e82eb",
            "6509b5d0283f40db90c76abcc3faceaa",
            "673d25e2dc7541c3bcf67629b8456da7",
            "972b7b50162b478ea63df2d349c21b55",
            "7938d1a1b05c45fc8da7e18c4382c62d",
            "a091e243a28b45fc912317eed2963301",
            "b4db034947a54a1cac8b0a1f3f7653cf",
            "de00e83bc6ca4de58db813cbca8ecd85",
            "1a4e77f78f9c4af499ed31bb94aa5c0d",
            "6083e5164a30407cbec7565213637d1c",
            "a9df432afc154d6fa5f80763bc61d31b",
            "8704b31feb3e4d7ebe9eb065cadba3f0",
            "838972df6ebf425187200086339fa60c",
            "4465cfd2f9c24c0bbf69561d30c1076b",
            "7eafcaa58c704b97a9715b15e7cd1d5d",
            "81e69396bf7d4c9ba22c728856030dbd",
            "673a69f55c8a45d4a95f05d4eb264f0d",
            "c413c949a15c4fd598de2f5eeb63462f",
            "024eba2be514485dbec727541cc5923b",
            "08e2f57373f74941b24459036a7fe87b",
            "4f4720e650a04b06909626a3ec33ed06",
            "15776d5085fd4ea2ab3dc882b0940124",
            "02dad75d2e6946469dd1a7ce8a946ce9",
            "7fbda44a2c1542aeaf6385df02180aa8",
            "98490016b098455096e0bd7e71eb00a0",
            "d30469fd940b425bb757eeae5837e79f",
            "0f506581c5f34250906805ddd89e439e",
            "8c260674af3c4c02abb42772a23feaa7"
          ]
        },
        "id": "pSqrwIrjAY84",
        "outputId": "de51f223-9654-465c-9c0a-8001e6fdcf8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.12.9: Fast Qwen2_5_Vl patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2178db6ecc064230a79c96598ff10053",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/6.90G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bd66e331a03454c8e38f441c90c8759",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7474f1cb8b9d4042b78f94cf6e2f6cd7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60f386cb2d6f40f690d791d6d3ef0cb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e36d89ef03a47cda53a4c3f18274e37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1455a7398714532bb13d897d8b74fbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ffaf5a68ae54cabb62612532a929c31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cd84c729a1649a98dabe8a8c21602be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "624329dfacb34a5abe9681884607ab4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa8121ed867044058aaf0b41a29c112c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.jinja: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4db034947a54a1cac8b0a1f3f7653cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "video_preprocessor_config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c413c949a15c4fd598de2f5eeb63462f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2_5_VLForConditionalGeneration(\n",
              "  (model): Qwen2_5_VLModel(\n",
              "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
              "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
              "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
              "      )\n",
              "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
              "      (blocks): ModuleList(\n",
              "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
              "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
              "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
              "          (attn): Qwen2_5_VLVisionAttention(\n",
              "            (qkv): lora.Linear(\n",
              "              (base_layer): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (proj): lora.Linear(\n",
              "              (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "          )\n",
              "          (mlp): Qwen2_5_VLMLP(\n",
              "            (gate_proj): lora.Linear(\n",
              "              (base_layer): Linear(in_features=1280, out_features=3420, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (up_proj): lora.Linear(\n",
              "              (base_layer): Linear(in_features=1280, out_features=3420, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (down_proj): lora.Linear(\n",
              "              (base_layer): Linear(in_features=3420, out_features=1280, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (merger): Qwen2_5_VLPatchMerger(\n",
              "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
              "        (mlp): Sequential(\n",
              "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (language_model): Qwen2_5_VLTextModel(\n",
              "      (embed_tokens): Embedding(152064, 3584)\n",
              "      (layers): ModuleList(\n",
              "        (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
              "          (self_attn): Qwen2_5_VLAttention(\n",
              "            (q_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (k_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (v_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (o_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
              "          )\n",
              "          (mlp): Qwen2MLP(\n",
              "            (gate_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (up_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (down_proj): lora.Linear4bit(\n",
              "              (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "              (lora_dropout): ModuleDict(\n",
              "                (default): Identity()\n",
              "              )\n",
              "              (lora_A): ModuleDict(\n",
              "                (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "              )\n",
              "              (lora_B): ModuleDict(\n",
              "                (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "              )\n",
              "              (lora_embedding_A): ParameterDict()\n",
              "              (lora_embedding_B): ParameterDict()\n",
              "              (lora_magnitude_vector): ModuleDict()\n",
              "            )\n",
              "            (act_fn): SiLUActivation()\n",
              "          )\n",
              "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        )\n",
              "      )\n",
              "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth import FastVisionModel\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2.5-VL-7B-Instruct\",\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model.load_adapter(\n",
        "    \"/content/drive/MyDrive/Cv/fake_image_detector_99\" # this should be the path to lora model trained\n",
        ")\n",
        "\n",
        "FastVisionModel.for_inference(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X70Hl_JCAY84"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "def predict_image(image_path, prompt=\"Real/Fake?\"):\n",
        "    \"\"\"\n",
        "    Perform inference on an image and return probabilities.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"Error: File {image_path} not found.\")\n",
        "        return None\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens = False,\n",
        "        return_tensors = \"pt\",\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=4,\n",
        "        use_cache=True,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    actual_tokenizer = tokenizer.tokenizer\n",
        "\n",
        "    # Get token IDs for \"Fake\" and \"Real\"\n",
        "    fake_token_id = actual_tokenizer.encode(\"Fake\", add_special_tokens=False)[0]\n",
        "    real_token_id = actual_tokenizer.encode(\"Real\", add_special_tokens=False)[0]\n",
        "\n",
        "    generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "    generated_text = actual_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Generated text: {generated_text}\")\n",
        "\n",
        "    first_token_logits = outputs.scores[0][0]  # [vocab_size]\n",
        "    probabilities = F.softmax(first_token_logits, dim=-1)\n",
        "\n",
        "    fake_prob = probabilities[fake_token_id].item()\n",
        "    real_prob = probabilities[real_token_id].item()\n",
        "\n",
        "    return {\n",
        "        \"Fake\": fake_prob,\n",
        "        \"Real\": real_prob\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpyHVPMoAY84"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_predict(image_path):\n",
        "    if not image_path:\n",
        "        return \"Please upload an image.\"\n",
        "    return predict_image(image_path)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# AI-Generated Image Detection - CMP 722\")\n",
        "    gr.Markdown(\"Upload an image to check if it is **Real** or **Fake**.\")\n",
        "    # For Method 2, Fake class below 80 should be counted as Real because of the threshold mentioned in report.\n",
        "    with gr.Row():\n",
        "        inp = gr.Image(type=\"filepath\", label=\"Upload Image\")\n",
        "        out = gr.Label(num_top_classes=2, label=\"Model Prediction\")\n",
        "\n",
        "    btn = gr.Button(\"Run Inference\")\n",
        "    btn.click(fn=gradio_predict, inputs=inp, outputs=out)\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ooy6y4X2AY84"
      },
      "outputs": [],
      "source": [
        "image_file = \"test_image.jpg\"\n",
        "\n",
        "if not os.path.exists(image_file):\n",
        "    Image.new('RGB', (512, 512), color='blue').save(image_file)\n",
        "\n",
        "print(f\"Processing {image_file}...\")\n",
        "result = predict_image(image_file)\n",
        "print(\"-\" * 30)\n",
        "print(\"Model Prediction:\", result)\n",
        "print(\"-\" * 30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
